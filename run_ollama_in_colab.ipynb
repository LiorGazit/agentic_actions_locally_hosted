{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0MJe6AusVtI"
      },
      "source": [
        "## Run Ollama in Colab\n",
        "[List of available Ollama LLMs.](https://ollama.com/library)  \n",
        "\n",
        "#### Here's a list of isses to take care of:\n",
        "1. Insert a Colab badge.  \n",
        "1. Mention here that it only works on Colab so far, not Windows.  \n",
        "1. Break this notebook down to separate .py files to be sourced.  \n",
        "1. **Managing Ollama Server Lifecycles:**\n",
        "    Currently, you use a background process (ollama serve). Consider a controlled lifecycle using Docker containers or managed processes (e.g., via supervisord or systemd).   \n",
        "1. [x] Apply \"**Explicit Error Handling**\" for each of the shell commands (see chat)  \n",
        "1. [ ] **Resource Monitoring & Logging:**  \n",
        "    Capture and monitor resource utilization (CPU/GPU, memory usage) to ensure sustainable performance.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-kJWXK4vYy8",
        "outputId": "edb819e4-768f-46fc-da3d-dabd211e79f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ollama not found, installing...\n",
            "Starting Ollama server...\n",
            "Started Ollama process with PID: 3287\n",
            "Waiting for Ollama server to be ready...\n",
            "Ollama server is ready.\n",
            "Pulling 'mistral-small' LLM model...\n",
            "Available models:\n",
            "NAME                    ID              SIZE     MODIFIED               \n",
            "mistral-small:latest    8039dd90c113    14 GB    Less than a second ago    \n",
            "\n",
            "Installing langchain-ollama via pip.\n",
            "LLM setup complete and ready for use.\n"
          ]
        }
      ],
      "source": [
        "# This is a .py script to be sourced: (I will need to format it as a separate file to be called with a variable llm_name)\n",
        "import shutil\n",
        "import subprocess\n",
        "from time import sleep\n",
        "import requests\n",
        "\n",
        "# Choice of LLM:\n",
        "llm_name = \"mistral-small\"  # \"mistral\"\n",
        "\n",
        "# Install Ollama via shell\n",
        "if shutil.which('ollama') is None:\n",
        "    print(\"Ollama not found, installing...\")\n",
        "    shell_output_curl_command = subprocess.run(\n",
        "        'curl https://ollama.ai/install.sh | sh',\n",
        "        capture_output=True, text=True, shell=True\n",
        "    )\n",
        "    if shell_output_curl_command.returncode != 0:\n",
        "        raise RuntimeError(f\"Error installing Ollama: {shell_output_curl_command.stderr}\")\n",
        "else:\n",
        "    print(\"Ollama is already installed.\")\n",
        "\n",
        "# Start Ollama server in background\n",
        "print(\"Starting Ollama server...\")\n",
        "process_serve = subprocess.Popen(\n",
        "    'OLLAMA_HOST=127.0.0.1:11434 ollama serve > serve.log 2>&1 &',\n",
        "    stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True\n",
        ")\n",
        "print(f\"Started Ollama process with PID: {process_serve.pid}\")\n",
        "\n",
        "# Function to ensure Ollama is ready\n",
        "def wait_for_ollama_ready(timeout=15):\n",
        "    print(\"Waiting for Ollama server to be ready...\")\n",
        "    for _ in range(timeout):\n",
        "        try:\n",
        "            response = requests.get(\"http://localhost:11434\")\n",
        "            if response.status_code == 200:\n",
        "                print(\"Ollama server is ready.\")\n",
        "                return\n",
        "        except requests.exceptions.ConnectionError:\n",
        "            sleep(1)\n",
        "    raise RuntimeError(\"Ollama server failed to start within timeout.\")\n",
        "\n",
        "wait_for_ollama_ready()\n",
        "\n",
        "# Pull LLM model\n",
        "print(f\"Pulling '{llm_name}' LLM model...\")\n",
        "shell_output_pull_LLM = subprocess.run(\n",
        "    f'ollama pull {llm_name}', capture_output=True, text=True, shell=True\n",
        ")\n",
        "if shell_output_pull_LLM.returncode != 0:\n",
        "    raise RuntimeError(f\"Error pulling '{llm_name}': {shell_output_pull_LLM.stderr}\")\n",
        "\n",
        "# Verify available models\n",
        "shell_output_models_list = subprocess.run(\n",
        "    'ollama list', capture_output=True, text=True, shell=True\n",
        ")\n",
        "if shell_output_models_list.returncode != 0:\n",
        "    raise RuntimeError(f\"Error listing models: {shell_output_models_list.stderr}\")\n",
        "else:\n",
        "    print(f\"Available models:\\n{shell_output_models_list.stdout}\")\n",
        "\n",
        "# Install LangChain Ollama integration\n",
        "print(\"Installing langchain-ollama via pip.\")\n",
        "pip_langchainollama_command = subprocess.run(\n",
        "    'pip install -U langchain-ollama', capture_output=True, text=True, shell=True\n",
        ")\n",
        "if pip_langchainollama_command.returncode != 0:\n",
        "    raise RuntimeError(f\"Error installing 'langchain-ollama': {pip_langchainollama_command.stderr}\")\n",
        "\n",
        "# Import and configure LLM\n",
        "from langchain_ollama.llms import OllamaLLM\n",
        "model = OllamaLLM(model=llm_name)\n",
        "\n",
        "print(\"LLM setup complete and ready for use.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5o4EfODWYmy",
        "outputId": "42c5f815-27f4-45c5-bbbb-ce8f51f7eb26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A good way to continue that sentence could be:\n",
            "\n",
            "\"You are a friend.\"\n",
            "\n",
            "Or\n",
            "\n",
            "\"You are a teacher.\"\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Provide concise and simple answer!\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = prompt | model\n",
        "\n",
        "print(chain.invoke({\"question\": \"What is a good way to continue this sentence: 'you are a ...'? It has to by syntactically correct!\"}))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
