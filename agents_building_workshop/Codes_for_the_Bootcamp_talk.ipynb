{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codes for bootcamp talk: Advanced LLM & Agent Systems Bootcamp\n",
    "By: Lior Gazit.  \n",
    "Repo: [agentic_actions_locally_hosted](https://github.com/LiorGazit/agentic_actions_locally_hosted)  \n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/LiorGazit/agentic_actions_locally_hosted/blob/main/agents_building_workshop/Codes_for_the_Bootcamp_talk.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a> (pick a GPU Colab session for fastest computing)  \n",
    "\n",
    "```\n",
    "Disclaimer: The content and ideas presented in this notebook are solely those of the author, Lior Gazit, and do not represent the views or intellectual property of the author's employer.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m108.2/108.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m438.1/438.1 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m306.4/306.4 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip -q install sentence-transformers faiss-cpu langchain tiktoken langsmith langchain_openai -U \"autogen-agentchat\" \"autogen-ext[openai]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this notebook is run outside of the repo's codes, get the necessary code from the remote repo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded spin_up_LLM.py from GitHub\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# If the module isn't already present (e.g. in Colab), fetch it from GitHub\n",
    "if not os.path.exists(\"spin_up_LLM.py\"):\n",
    "    url = \"https://raw.githubusercontent.com/LiorGazit/agentic_actions_locally_hosted/refs/heads/main/spin_up_LLM.py\"\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    with open(\"spin_up_LLM.py\", \"w\") as f:\n",
    "        f.write(resp.text)\n",
    "    print(\"Downloaded spin_up_LLM.py from GitHub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 1: RAG Pipeline with Chained Prompt Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the Vector Store and RAG pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8551efc90c9c43bf86a27e1045f72727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ec12513a50493490e168617ccb16cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83c6fdcfa7f49f49139e29851edc1ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468707cbd7d0437f82c47c9ee2139a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e3f362ff174c10ad8c936d9664f59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3776f9494111402faa94f64a4164693b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8136d05064414c8ff4b78ff8287485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88e709aa53e47328089ddec9bc682ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a526e79c094a3da5945bf889ee5070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a72932c02058426a947e5cc79c44eec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24bdd36f84ef43a5b094caec29c83223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Retrieved documents for context:\n",
      " [\"The company's earnings call mentioned concerns over increased production costs.\", 'Recent financial filings show revenue growth despite supply chain issues.']\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Example documents (could be clinical notes, financial filings, etc.)\n",
    "documents = [\n",
    "    \"Patient has diabetes type 2 and shows high glucose levels.\",\n",
    "    \"Recent financial filings show revenue growth despite supply chain issues.\",\n",
    "    \"Patient diagnosed with hypertension, recommended lifestyle changes.\",\n",
    "    \"The company's earnings call mentioned concerns over increased production costs.\",\n",
    "]\n",
    "\n",
    "# Step 1: Create embeddings using SentenceTransformer\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # lightweight embedding model\n",
    "\n",
    "# Generate embeddings for the documents\n",
    "document_embeddings = embedding_model.encode(documents)\n",
    "\n",
    "# Step 2: Setup FAISS vector store\n",
    "dimension = document_embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(document_embeddings)\n",
    "\n",
    "# Step 3: Define the retriever(query) function\n",
    "def retriever(query, top_k=2):\n",
    "    # Generate embedding for the query\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "\n",
    "    # Perform the similarity search in the FAISS index\n",
    "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
    "\n",
    "    # Retrieve the top_k most similar documents\n",
    "    retrieved_docs = [documents[idx] for idx in indices[0]]\n",
    "\n",
    "    return retrieved_docs\n",
    "\n",
    "# Example usage of the retriever\n",
    "query = \"What did the company say about production costs?\"\n",
    "context_docs = retriever(query)\n",
    "print(\"\\n\\nRetrieved documents for context:\\n\", context_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompting using a locally hosted LLM via Ollama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Installing Ollama...\n",
      "ğŸš€ Starting Ollama server...\n",
      "â†’ Ollama PID: 1335\n",
      "â³ Waiting for Ollama to be readyâ€¦\n",
      "ğŸš€ Pulling model 'gemma3'â€¦\n",
      "Available models:\n",
      "NAME             ID              SIZE      MODIFIED               \n",
      "gemma3:latest    a2af6cc3eb7f    3.3 GB    Less than a second ago    \n",
      "\n",
      "ğŸš€ Installing langchain-ollamaâ€¦\n",
      "A: The patient has diabetes type 2 and hypertension.\n"
     ]
    }
   ],
   "source": [
    "from spin_up_LLM import spin_up_LLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "query = \"What is the patient's diagnosis given these notes?\"\n",
    "context_docs = retriever(query)\n",
    "question = f\"Using the following context, answer the question:\\n\\n{context_docs}\\n\\nQ: {query}\\n\\n---\\nA:\"\n",
    "local_llm = spin_up_LLM(chosen_llm=\"gemma3\")\n",
    "\n",
    "answer_local = local_llm.generate([question])\n",
    "print(answer_local.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompting using OpenAI's API (paid) route:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paste your OpenAI API key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
      "The patient's diagnosis is type 2 diabetes and hypertension.\n"
     ]
    }
   ],
   "source": [
    "# In Colab, use getpass to securely prompt for your API key\n",
    "from getpass import getpass\n",
    "import openai\n",
    "\n",
    "openai.api_key = getpass(\"Paste your OpenAI API key: \")\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\":\"system\",\"content\":\"You are a medical assistant.\"},\n",
    "        {\"role\":\"user\",  \"content\": question}\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_api = response.choices[0].message.content\n",
    "print(answer_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 2: Multi-Agent Team Interaction (Agent Collaboration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting Ollama server...\n",
      "â†’ Ollama PID: 1653\n",
      "â³ Waiting for Ollama to be readyâ€¦\n",
      "ğŸš€ Pulling model 'CodeLlama'â€¦\n",
      "Available models:\n",
      "NAME                ID              SIZE      MODIFIED               \n",
      "CodeLlama:latest    8fdf8f752f6e    3.8 GB    Less than a second ago    \n",
      "gemma3:latest       a2af6cc3eb7f    3.3 GB    About a minute ago        \n",
      "\n",
      "ğŸš€ Installing langchain-ollamaâ€¦\n",
      "ğŸš€ Starting Ollama server...\n",
      "â†’ Ollama PID: 1896\n",
      "â³ Waiting for Ollama to be readyâ€¦\n",
      "ğŸš€ Pulling model 'Llama2'â€¦\n",
      "Available models:\n",
      "NAME                ID              SIZE      MODIFIED               \n",
      "Llama2:latest       78e26419b446    3.8 GB    Less than a second ago    \n",
      "CodeLlama:latest    8fdf8f752f6e    3.8 GB    59 seconds ago            \n",
      "gemma3:latest       a2af6cc3eb7f    3.3 GB    2 minutes ago             \n",
      "\n",
      "ğŸš€ Installing langchain-ollamaâ€¦\n",
      "Coder's output:\n",
      " \n",
      "def is_prime(n):\n",
      "    \"\"\" Checks if the input number 'n' is a prime number\"\"\"\n",
      "    if n < 2:\n",
      "        return False\n",
      "    for i in range(2, int(n ** 0.5) + 1):\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "\n",
      "Reviewer's feedback:\n",
      " As a reviewer, I would like to provide some feedback on the provided code. Here are some points to consider:\n",
      "\n",
      "1. The function `is_prime` seems to be well-structured and easy to understand. However, it does not handle the base case correctly. In the current implementation, if the input number is less than 2, the function returns `False`, but it should return `True` instead since every number less than 2 is already known to be prime.\n",
      "2. The loop to find potential factors of the input number is correct, but it could be optimized slightly. Instead of checking each number from 2 to the square root of the input number, you could check the square root directly and then check the remaining numbers below it. This would reduce the number of checks and make the algorithm more efficient.\n",
      "3. The function does not handle composite numbers correctly. If a number is composite, it should return `False` immediately without checking its factors. Currently, if a number is composite, the function will continue to check its factors even though it already knows the answer.\n",
      "4. The function does not provide any information about the primality of the input number. It only returns `True` or `False`, which does not provide any context or additional information about the result. Consider adding some additional output or logging to provide more information about the primality of the input number.\n",
      "\n",
      "Based on these considerations, I would approve the function with some minor modifications to handle the base case and optimize the algorithm for better efficiency.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: this code cell runs for ~3 minutes on a Google Colab free GPU session, but ~15 minutes in a Google Colab free CPU session!\n",
    "coder = spin_up_LLM(chosen_llm=\"CodeLlama\")  # or OpenAI model\n",
    "reviewer = spin_up_LLM(chosen_llm=\"Llama2\")  # a more general model for critique\n",
    "\n",
    "task = \"Write a Python function to check if a number is prime.\"\n",
    "conversation = []\n",
    "# Initialize conversation\n",
    "conversation.append((\"System\", \"Agents: collaborate to solve the task. Coder writes code, Reviewer suggests fixes.\"))\n",
    "conversation.append((\"User\", task))\n",
    "\n",
    "# Agent A (Coder) turn\n",
    "code_response = coder.generate([f\"Task: {task}\\nRole: Coder\\nYou are a coding agent. Provide code only.\\n\"])\n",
    "conversation.append((\"Coder\", code_response.generations[0][0].text))\n",
    "\n",
    "# Agent B (Reviewer) turn\n",
    "review_response = reviewer.generate([f\"Code:\\n{code_response}\\nRole: Reviewer\\nYou are a code reviewer. Provide feedback or approve.\\n\"])\n",
    "conversation.append((\"Reviewer\", review_response.generations[0][0].text))\n",
    "\n",
    "print(\"Coder's output:\\n\", code_response.generations[0][0].text)\n",
    "print(\"\\nReviewer's feedback:\\n\", review_response.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, here is an example using AutoGen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autogen-agentchat in /usr/local/lib/python3.11/dist-packages (0.6.1)\n",
      "Requirement already satisfied: autogen-ext[openai] in /usr/local/lib/python3.11/dist-packages (0.6.1)\n",
      "Requirement already satisfied: autogen-core==0.6.1 in /usr/local/lib/python3.11/dist-packages (from autogen-agentchat) (0.6.1)\n",
      "Requirement already satisfied: jsonref~=1.1.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.6.1->autogen-agentchat) (1.1.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.27.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.6.1->autogen-agentchat) (1.34.0)\n",
      "Requirement already satisfied: pillow>=11.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.6.1->autogen-agentchat) (11.2.1)\n",
      "Requirement already satisfied: protobuf~=5.29.3 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.6.1->autogen-agentchat) (5.29.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.6.1->autogen-agentchat) (2.11.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.6.1->autogen-agentchat) (4.14.0)\n",
      "Requirement already satisfied: aiofiles in /usr/local/lib/python3.11/dist-packages (from autogen-ext[openai]) (24.1.0)\n",
      "Requirement already satisfied: openai>=1.66.5 in /usr/local/lib/python3.11/dist-packages (from autogen-ext[openai]) (1.84.0)\n",
      "Requirement already satisfied: tiktoken>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from autogen-ext[openai]) (0.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.66.5->autogen-ext[openai]) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.66.5->autogen-ext[openai]) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.66.5->autogen-ext[openai]) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.66.5->autogen-ext[openai]) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.66.5->autogen-ext[openai]) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>=1.66.5->autogen-ext[openai]) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.8.0->autogen-ext[openai]) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.8.0->autogen-ext[openai]) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai>=1.66.5->autogen-ext[openai]) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.66.5->autogen-ext[openai]) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.66.5->autogen-ext[openai]) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.66.5->autogen-ext[openai]) (0.16.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.27.0->autogen-core==0.6.1->autogen-agentchat) (8.7.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.6.1->autogen-agentchat) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.6.1->autogen-agentchat) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.6.1->autogen-agentchat) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken>=0.8.0->autogen-ext[openai]) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken>=0.8.0->autogen-ext[openai]) (2.4.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.27.0->autogen-core==0.6.1->autogen-agentchat) (3.22.0)\n"
     ]
    }
   ],
   "source": [
    "!pip -q install -U \"autogen-agentchat\" \"autogen-ext[openai]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Coderâ€™s Output ===\n",
      "\n",
      "Write a Python function `is_prime(n)` that returns True if `n` is prime.\n",
      "```python\n",
      "def is_prime(n):\n",
      "    if n <= 1:\n",
      "        return False\n",
      "    if n <= 3:\n",
      "        return True\n",
      "    if n % 2 == 0 or n % 3 == 0:\n",
      "        return False\n",
      "    i = 5\n",
      "    while i * i <= n:\n",
      "        if n % i == 0 or n % (i + 2) == 0:\n",
      "            return False\n",
      "        i += 6\n",
      "    return True\n",
      "```\n",
      "\n",
      "=== Reviewerâ€™s Feedback ===\n",
      "\n",
      "Review the following code for correctness and edge cases:\n",
      "\n",
      "messages=[TextMessage(source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 10, 3, 23, 53, 111247, tzinfo=datetime.timezone.utc), content='Write a Python function `is_prime(n)` that returns True if `n` is prime.', type='TextMessage'), TextMessage(source='Coder', models_usage=RequestUsage(prompt_tokens=43, completion_tokens=102), metadata={}, created_at=datetime.datetime(2025, 6, 10, 3, 23, 54, 508391, tzinfo=datetime.timezone.utc), content='```python\\ndef is_prime(n):\\n    if n <= 1:\\n        return False\\n    if n <= 3:\\n        return True\\n    if n % 2 == 0 or n % 3 == 0:\\n        return False\\n    i = 5\\n    while i * i <= n:\\n        if n % i == 0 or n % (i + 2) == 0:\\n            return False\\n        i += 6\\n    return True\\n```', type='TextMessage')] stop_reason=None\n",
      "The code provided is a list of `TextMessage` objects, each containing metadata and content related to a conversation about writing a Python function `is_prime(n)`. The function `is_prime(n)` is included in the content of one of the messages. Let's review the function for correctness and potential edge cases:\n",
      "\n",
      "### Function Review\n",
      "\n",
      "```python\n",
      "def is_prime(n):\n",
      "    if n <= 1:\n",
      "        return False\n",
      "    if n <= 3:\n",
      "        return True\n",
      "    if n % 2 == 0 or n % 3 == 0:\n",
      "        return False\n",
      "    i = 5\n",
      "    while i * i <= n:\n",
      "        if n % i == 0 or n % (i + 2) == 0:\n",
      "            return False\n",
      "        i += 6\n",
      "    return True\n",
      "```\n",
      "\n",
      "### Correctness\n",
      "\n",
      "1. **Basic Checks**: \n",
      "   - The function correctly returns `False` for numbers less than or equal to 1, as these are not prime.\n",
      "   - It returns `True` for numbers 2 and 3, which are prime.\n",
      "\n",
      "2. **Divisibility Checks**:\n",
      "   - The function correctly checks for divisibility by 2 and 3, returning `False` if `n` is divisible by either.\n",
      "\n",
      "3. **Loop for Larger Numbers**:\n",
      "   - The loop starts at 5 and checks divisibility for numbers of the form `6k Â± 1`, which is a common optimization for checking primality.\n",
      "   - The loop condition `i * i <= n` is correct, as it only needs to check up to the square root of `n`.\n",
      "\n",
      "### Edge Cases\n",
      "\n",
      "1. **Negative Numbers**: The function correctly returns `False` for negative numbers since they are not prime.\n",
      "\n",
      "2. **Zero and One**: The function correctly returns `False` for 0 and 1.\n",
      "\n",
      "3. **Small Primes**: The function correctly identifies small prime numbers like 2 and 3.\n",
      "\n",
      "4. **Even Numbers Greater Than 2**: The function correctly returns `False` for even numbers greater than 2.\n",
      "\n",
      "5. **Large Primes**: The function should correctly identify large prime numbers due to the efficient loop structure.\n",
      "\n",
      "6. **Performance**: The function is efficient for reasonably large values of `n` due to the `6k Â± 1` optimization.\n",
      "\n",
      "### Potential Improvements\n",
      "\n",
      "- **Type Checking**: The function does not check if `n` is an integer. If a non-integer is passed, it may result in unexpected behavior or errors.\n",
      "- **Documentation**: Adding a docstring to explain the function's purpose, parameters, and return value would improve readability and maintainability.\n",
      "\n",
      "Overall, the function `is_prime(n)` is implemented correctly and handles the typical edge cases for checking primality.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "\n",
    "# 1. Import the agent classes and the OpenAI client\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "async def multi_agent_demo():\n",
    "    # 2. Configure your OpenAI API key\n",
    "    api_key = openai.api_key\n",
    "    if not api_key:\n",
    "        openai.api_key = getpass(\"Paste your OpenAI API key: \")\n",
    "\n",
    "    # 3. Create the OpenAI model client\n",
    "    model_client = OpenAIChatCompletionClient(\n",
    "        model=\"gpt-4o\",\n",
    "        api_key=api_key,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    # 4. Instantiate two LLM agents with distinct roles\n",
    "    coder = AssistantAgent(\n",
    "        name=\"Coder\",\n",
    "        model_client=model_client,\n",
    "        system_message=\"You are a Python coding assistant. Produce only working code.\"\n",
    "    )\n",
    "    reviewer = AssistantAgent(\n",
    "        name=\"Reviewer\",\n",
    "        model_client=model_client,\n",
    "        system_message=\"You are a code reviewer. Point out bugs or edge cases.\"\n",
    "    )\n",
    "\n",
    "    # 5. Coder agent writes a function\n",
    "    code_task = \"Write a Python function `is_prime(n)` that returns True if `n` is prime.\"\n",
    "    code = await coder.run(task=code_task)\n",
    "    print(\"=== Coderâ€™s Output ===\\n\")\n",
    "    for msg in code.messages:\n",
    "        print(msg.content)\n",
    "\n",
    "    # 6. Reviewer agent critiques the code\n",
    "    review = await reviewer.run(task=f\"Review the following code for correctness and edge cases:\\n\\n{code}\")\n",
    "    print(\"\\n=== Reviewerâ€™s Feedback ===\\n\")\n",
    "    for msg in review.messages:\n",
    "        print(msg.content)\n",
    "\n",
    "    # 7. Clean up\n",
    "    await model_client.close()\n",
    "\n",
    "# 8. Execute the multiâ€‘agent demo\n",
    "await multi_agent_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 3: Monitoring & Tracing Example, and Model Differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code example for where building the logging process ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting Ollama server...\n",
      "â†’ Ollama PID: 2475\n",
      "â³ Waiting for Ollama to be readyâ€¦\n",
      "ğŸš€ Pulling model 'CodeLlama'â€¦\n",
      "Available models:\n",
      "NAME                ID              SIZE      MODIFIED               \n",
      "CodeLlama:latest    8fdf8f752f6e    3.8 GB    Less than a second ago    \n",
      "Llama2:latest       78e26419b446    3.8 GB    About a minute ago        \n",
      "gemma3:latest       a2af6cc3eb7f    3.3 GB    3 minutes ago             \n",
      "\n",
      "ğŸš€ Installing langchain-ollamaâ€¦\n",
      "=== Coderâ€™s Code ===\n",
      " [PYTHON]\n",
      "def reverse_string(s):\n",
      "    return s[::-1]\n",
      "[/PYTHON]\n",
      "[TESTS]\n",
      "# Test case 1:\n",
      "assert reverse_string(\"hello\") == \"olleh\"\n",
      "# Test case 2:\n",
      "assert reverse_string(\"\") == \"\"\n",
      "# Test case 3:\n",
      "assert reverse_string(\"a\") == \"a\"\n",
      "# Test case 4:\n",
      "assert reverse_string(\"ab\") == \"ba\"\n",
      "# Test case 5:\n",
      "assert reverse_string(\"abc\") == \"cba\"\n",
      "[/TESTS]\n",
      "\n",
      "\n",
      "=== Reviewerâ€™s Feedback ===\n",
      " \n",
      "The provided code is a correct implementation of a reverse string function in Python. It takes in a single argument `s` which is the input string to be reversed, and returns the reversed string.\n",
      "\n",
      "Here are some test cases that demonstrate the functionality of the reverse_string function:\n",
      "\n",
      "* Test case 1: AssertionError\n",
      "\t+ Input: \"hello\"\n",
      "\t+ Output: \"olleh\" (correct)\n",
      "* Test case 2: \"\"\n",
      "\t+ Input: \"\"\n",
      "\t+ Output: \"\" (correct)\n",
      "* Test case 3: \"a\"\n",
      "\t+ Input: \"a\"\n",
      "\t+ Output: \"a\" (correct)\n",
      "* Test case 4: \"ab\"\n",
      "\t+ Input: \"ab\"\n",
      "\t+ Output: \"ba\" (correct)\n",
      "* Test case 5: \"abc\"\n",
      "\t+ Input: \"abc\"\n",
      "\t+ Output: \"cba\" (correct)\n",
      "\n",
      "These test cases demonstrate that the reverse_string function works correctly for all edge cases, including an empty string and a single-character input.\n",
      "\n",
      "Overall, this code is well-written and easy to understand. The use of docstrings and comments makes it clear what the purpose of the code is and how it works. Additionally, the test cases provide confidence that the function works as intended.\n",
      "\n",
      "---\n",
      "Printing the log:\n",
      "{\"step\": \"Coder\", \"prompt_tokens\": 16, \"response_tokens\": 102, \"duration_s\": 21.213, \"timestamp\": 1749525848.0459435}\n",
      "{\"step\": \"Reviewer\", \"prompt_tokens\": 944, \"response_tokens\": 253, \"duration_s\": 11.038, \"timestamp\": 1749525870.1218917}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import logging\n",
    "import json\n",
    "import tiktoken\n",
    "\n",
    "# 1. Basic logging setup\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(message)s\")\n",
    "\n",
    "# 2. Helper: count tokens using tiktoken\n",
    "def count_tokens(text, encoding_name=\"cl100k_base\"):\n",
    "    enc = tiktoken.get_encoding(encoding_name)\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "# 3. Helper: log each call\n",
    "def log_call(step_name, prompt, response, start, end, log_file=\"llm_trace.log\"):\n",
    "    record = {\n",
    "        \"step\": step_name,\n",
    "        \"prompt_tokens\": count_tokens(prompt),\n",
    "        \"response_tokens\": count_tokens(response),\n",
    "        \"duration_s\": round(end - start, 3),\n",
    "        \"timestamp\": start\n",
    "    }\n",
    "    # Console output\n",
    "    logging.info(f\"[{step_name}] {record['duration_s']}s | \"\n",
    "                 f\"prompt_tokens={record['prompt_tokens']} | \"\n",
    "                 f\"response_tokens={record['response_tokens']}\")\n",
    "    # Append to JSONâ€‘lines file\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "# 4. Load your model (local Ollama example)\n",
    "model = spin_up_LLM(chosen_llm=\"CodeLlama\")\n",
    "\n",
    "# 5. StepÂ 1: Coder agent (generate code)\n",
    "step1_prompt = \"Write a Python function `reverse_string(s)` that returns the reverse of s.\"\n",
    "start = time.time()\n",
    "step1_response = model.generate([step1_prompt])\n",
    "end = time.time()\n",
    "log_call(\"Coder\", step1_prompt, step1_response.generations[0][0].text, start, end)\n",
    "\n",
    "# 6. StepÂ 2: Reviewer agent (review code)\n",
    "step2_prompt = f\"Review this code for correctness and edge cases:\\n\\n{step1_response}\"\n",
    "start = time.time()\n",
    "step2_response = model.generate([step2_prompt])\n",
    "end = time.time()\n",
    "log_call(\"Reviewer\", step2_prompt, step2_response.generations[0][0].text, start, end)\n",
    "\n",
    "# 7. Print outputs\n",
    "print(\"=== Coderâ€™s Code ===\\n\", step1_response.generations[0][0].text)\n",
    "print(\"\\n=== Reviewerâ€™s Feedback ===\\n\", step2_response.generations[0][0].text)\n",
    "\n",
    "# 8. Inspect the log file if desired:\n",
    "print(\"\\n---\\nPrinting the log:\")\n",
    "!head -n 10 llm_trace.log"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
